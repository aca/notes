todo:
    "Why Logical Clocks are Easy" https://queue.acm.org/detail.cfm?id=2917756
    "A Note on Distributed Computing" https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628
    https://blog.separateconcerns.com/2015-07-07-four-easy-reads-distsys.html
    https://www.microsoft.com/en-us/research/publication/byzantine-generals-problem/
      - the core, basic understanding needed to build distributed systems
      - Need lots of (consistent) metadata across all nodes before you can
        finalize a transaction. The metadata and how fast you could move it
        on the network is the constraint on how fast you can run transactions.
    http://www.infoq.com/presentations/latency-pitfalls
    http://web.mit.edu/2.75/resources/random/How%20Complex%20Systems%20Fail.pdf
    https://github.com/aphyr/distsys-class
    https://github.com/heathermiller/dist-prog-book
        RPC, Futures & Promises, Message-passing,  CAP, Consistency, & CRDTs, Large-scale Parallel Batch Processing, Large-scale Streaming,
    "Impossibility of consensus" http://courses.csail.mit.edu/6.852/05/papers/

    distribute systems theory:
        introductory (a "gem"):
            kttp://book.mixu.net/distsys/single-page.html
        practical:
            http://the-cloud-book.com/
        theoretical:
            http://dcg.ethz.ch/lectures/podc_allstars/

DISTRIBUTED SYSTEMS PRACTICES
  - Coordination is very hard.  https://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/
    Avoid coordinating machines wherever possible ("horizontal scalability").
    Independence: able to get data to machines such that communication and consensus between those machines is kept to a minimum.
    Whenever two machines must agree on something, the service becomes harder to implement.
    Information has an upper limit to the speed it can travel, and networked communication is flakier than you think, and your idea of what constitutes consensus is probably wrong.
  - “It’s slow” is the hardest problem you’ll ever debug.
  - Implement backpressure throughout your system.
    - Drop new messages (and increment a metric)
    - Ship errors back to users (and increment a metric)
    - Timeouts and exponential back-offs on connections/requests
  - Metrics are the only way to get your job done. Expose metrics (latency percentiles, increasing counters, rates of change)
    - Knowing how the system’s behavior on day 20 is different from its behavior
      on day 15 is the difference between successful engineering and failed
      shamanism.
    - Logs are often misleading. Metrics/experiments must back up the story.
  - Use percentiles, not averages. Percentiles (50th, 99th, 99.9th, 99.99th) are
    more accurate/informative than averages.
    - Using a mean assumes that the metric follows a bell curve.
  - Feature flags are how infrastructure is rolled out.
    - Accept that having multiple versions of infrastructure/data is a norm, not an rarity.
    - Trades local complexity (in the code, in one system) for global simplicity/resilience.
  - Choose id spaces wisely.
    - The more ids required to get to a piece of data, the more options you have
      in partitioning the data. The fewer ids required to get a piece of data,
      the easier it is to consume your system’s output.
  - Exploit data-locality.
    - Space-locality: the closer the processing and caching of your data is kept
      to its persistent storage, the more efficient your processing and caching.
    - Time-locality: If multiple users are making the same expensive request at
      nearly the same time, perhaps their requests can be joined into one.
  - Writing cached data back to persistent storage is bad.
  - Homework: apply CAP to critique Russian-doll caching.

DISTRIBUTED SYSTEMS TOOLBOX
  queue
    - break big messages into chunks! (e.g. S3 files)
  sharding
  zookeeper ("consensus system")
  map/reduce (ex: spark)
  pulsar: https://pulsar.apache.org  https://news.ycombinator.com/item?id=21936252
    "Kafka is dead"
    pub/sub (SQS) + log (Kinesis, Kafka)
                                  ^Kafka (AWS: Kinesis): real-time data pipeline/streaming. ordered, exactly-once messages. Horizontally scalable, fault-tolerant, fast.
  CDN
  LB: layer3 (ip-address), layer4 (DNS), layer7 (app)
  gossip protocol (ex: SWIM https://prakhar.me/articles/swim/ )
    - peer-to-peer virus-like communication amongst all members of an ad-hoc
      network (no central registry)
    - rely on each member to pass it along to their neighbors
    - expect packet loss and partitions
    - membership protocol answers "who are my peers?"
  sql vs nosql
    sql: ACID; scale horizontally by sharding
    nosql (cassandra, bigtable): efficiency, not correctness. write-heavy. eventually consistent.
  consistent hashing
  distributed lock
  distributed cache
  distributed datastore: each node has partial backup of some other node.
  replication
    leaderless replication = "Dynamo technique" https://en.wikipedia.org/wiki/Dynamo_(storage_system)
  eventual consistency

PRIMITIVES  https://www.the-paper-trail.org/post/2014-08-09-distributed-systems-theory-for-the-distributed-systems-engineer/
  - Leader election , e.g. http://en.wikipedia.org/wiki/Bully_algorithm
  - Consistent snapshotting  http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf
  - Consensus
    2PC https://the-paper-trail.org/blog/consensus-protocols-two-phase-commit/
    3PC https://the-paper-trail.org/blog/consensus-protocols-three-phase-commit/
  - Distributed state machine replication  http://en.wikipedia.org/wiki/State_machine_replication
    - Broadcast: delivering messages to more than one node at once
    - Atomic broadcast: can you deliver a message to all nodes in a group, or none?
    - Gossip
      classic paper: http://bitsavers.informatik.uni-stuttgart.de/pdf/xerox/parc/techReports/CSL-89-1_Epidemic_Algorithms_for_Replicated_Database_Maintenance.pdf
    - Causal multicast (but also consider the enjoyable back-and-forth between Birman and Cheriton).
  - Chain replication: neat way of ensuring consistency and ordering of writes
    by organizing nodes into a virtual linked list.
    - original paper: http://www.cs.cornell.edu/home/rvr/papers/OSDI04.pdf


http://ferd.ca/lessons-learned-while-working-on-large-scale-server-software.html
    Crash Early, Crash Often: Violent _early_ failure on 1/Nth of your system
    is far better than slow, silent corruption in 100% of it.

CAP:
    http://thislongrun.blogspot.com/2015/03/the-confusing-cap-and-acid-wording.html
    http://markburgess.org/blog_cap.html

    http://blog.foundationdb.com/minimal-explanation-of-the-cap-theorem
        - Choosing (C) only requires stopping the machines that got
          disconnected, not the whole system!
        - 'Availability' in CAP refers to "perfect availabilty", where _all_
          nodes are _always_ up.
        - By choosing (A) you don't really have one database anymore, you have
          two or more! Either the database or the application needs to
          reconcile this problem and eventually re-sync to get back to the
          normal state.
        - Choosing (A) excludes the possibility of ACID transactions.
        - There will only be incoming reads and writes to the disconnected
          computer if there are clients that are still connected to it (on the
          same side of the partition).

    http://ferd.ca/beating-the-cap-theorem-checklist.html
        - high latency is indistinguishable from splits or unavailability
        - there might be more than 1 partition at the same time
        - split nodes can vanish forever
        - a split node cannot be differentiated from a crashed one by its peers
        - clients are also part of the distributed system
        - clocks drift across multiple parts of the system, forward and backwards in time

        Common mistakes when trying to "beat" CAP:
            - your solution requires a central authority that cannot be unavailable
            - read-only mode is still unavailability for writes
            - your quorum size cannot be changed over time
            - your cluster size cannot be changed over time
            - you assume short periods of unavailability are insignificant

    🎅  http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html
        Incremental updates and the CAP theorem really don't play well together;
        mutable values require read-repair in an eventually consistent system.
        By rejecting incremental updates, embracing immutable data, and
        computing queries from scratch each time, you avoid that complexity. The
        CAP theorem has been beaten.

        Key properties of what a real solution will look like:
            1. The system makes it easy to store and scale an immutable,
               constantly-growing dataset
            2. The primary write operation is adding new immutable facts
            3. The system avoids the complexity of CAP by recomputing queries
               from raw data
            4. The system uses incremental algorithms to lower the latency of
               queries to an acceptable level

        Everything from here on out is optimization. Databases, indexing, ETL,
        batch computation, stream processing -- these are all techniques for
        optimizing query functions and bringing the latency down to an
        acceptable level.

distributed lock
    properties:
      1. mutex
      2. deadlock-free => TTL + monotonic fence
      3. fault-tolerant
    https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html
    > you need to include a fencing token with every write request: a number
    > that increases (e.g. incremented by the lock service) every time a client
    > acquires the lock. For example, if you are using ZooKeeper as lock
    > service, you can use the zxid or znode version number as fencing token.


zookeeper ("consensus system")  https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html
    ZK by default does not provide linearizable reads. Each client is connected
    to one of the server nodes, reads see only the data on that node, even if
    there are more up-to-date writes on another node. This makes reads much
    faster than if you had to assemble a quorum or contact the leader for every
    read, but it also means that ZK by default does not meet CAP-consistency.

    It is possible to make linearizable reads in ZK by preceding a read with
    a `sync` command. That isn’t the default though, because it comes with
    a performance penalty.

    What availability? Well, ZK requires a majority quorum in order to reach
    consensus, i.e. in order to process writes. In a partition, the majority
    side continues to function, but the minority side can’t process writes.
    Thus, writes in ZK are not CAP-available under a partition (even though the
    majority side can continue to process writes).

    Calling ZK “not consistent”, just because it’s not linearizable by default,
    badly misrepresents its features. It actually provides an excellent level of
    consistency! It provides _atomic broadcast_ (which is reducible to
    consensus) combined with the session guarantee of causal consistency.


Log ("journal" or "data log", not human-readable "text log"):
    https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying
    - Append-only, time-ordered sequence of records.
    - Each entry has a unique sequential number.
    convenient property: it is decoupled from a physical clock. Essential for distributed systems.
    solves two problems:
      - ordering changes
      - distributing data

      The log-centric approach to distributed systems arises from this State Machine Replication Principle:
        > If two identical, deterministic processes begin in the same state and
        > get the same inputs in the same order, they will produce the same output
        > and end in the same state.


    Sometimes called write-ahead logs or commit logs or transaction logs.
    For data integration, real time processing, and system building.



Paxos Made Simple: http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf
paxos: http://blog.willportnoy.com/2012/06/lessons-learned-from-paxos.html

raft: http://thesecretlivesofdata.com/raft/
    - leader is elected via a random timeout

testing/correctness
    - Lineage-driven Fault Injection (LDFI): observes what succeeds without
      faults, then induces targeted faults starting from the last successful
      operation, working backwards.
