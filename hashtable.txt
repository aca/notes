HASH FUNCTION
================================================================================
goal: "avalanche" with uniform distribution
    - cf. CRC, where the priority is _not_ uniformity of distribution, but to
      avoid collisions for _similar_ inputs.

key concepts:
    * "Rotate"/"Mix" the "internal state" to improve distribution, then XOR etc.
      * permute internal state such that each hash result has exactly one input
        that will produce it. Use _every part_ of the key to compute the hash.
    * Avalanche: output is wildly different if input changes only slightly
      (single bit).
        * This aids distribution: similar keys will _not_ have similar hash
          values => uniformly distributed => minimize collisions
    * Completeness (ideal avalanche): Hash function is "complete" if _each_
      output-bit depends on _all_ input-bits. If 1 bit of the input (plaintext)
      is changed, every bit of the output (ciphertext) has 50% probability of
      changing.


"Guidelines and rules for GetHashCode"
http://ericlippert.com/2011/02/28/guidelines-and-rules-for-gethashcode/
    > Multiplication is nothing more than repeated bit shifts and adds;
    > multiplying by 33 is just shifting by five bits and adding. Basically this
    > means "mess up the top 27 bits and keep the bottom 5 the same" in the hope
    > that the subsequent add will mess up the lower 5 bits. Multiplying by
    > a largish prime has the nice property that it messes up all the bits. I'm
    > not sure where the number 33 comes from though.

    > I suspect that prime numbers turn up in hash algorithms as much by tradition
    > and superstition as by science. Using a prime number as the modulus and
    > a different one as the multiplier can apparently help avoid clustering in
    > some scenarios.

AVOID specialized hash functions ("best for strings", "best for integers"); if
not good for all types of data, probably a poor algorithm.

BAD hash algo: Additive hash
  Any hash algorithm that relies primarily on a commutative (independent of
  order) operation will have bad distribution (e.g. abc/cba/cab hash to the same
  value).
BAD hash algo: XOR hash
  Better than additive hash, but not enough internal mixing.


HASHTABLE
================================================================================
Array-abstraction allowing any value to be used as an index. (cf. associative
array)

    // Get an index for the key.
    size_t idx = hash(key, size) % table->size;

Worst-case time of a hashtable lookup is bounded by the hash-collision strategy.
    - Linked-list ("chaining") is of course O(n)...
        - Delete is trivial.
        - Requires extra storage for pointers, and frustrates cache-locality.
          - But chaining can be improved by moving the match to front of list:
            "In-memory hash tables for accumulating text vocabularies"
            http://www.mathcs.emory.edu/~whalen/Hash/Hash_Articles/In-memory%20hash%20tables%20for%20accumulating%20text%20vocabularies.pdf
    - Open addressing: alt. to "chaining". Linear, Quadratic, Double-hashing...
        - Delete is a pain.
            - Some use a "mark" strategy, but then if many keys are deleted,
              scan-time gets terrible though the logical load-factor is low.
            - Thus the typical strategy is to rebuild the probe-sequence
              following the deleted item.
        - Worst-case is also O(n), but gains cache locality.
        - Double hashing: avoids clustering
            - "Cuckoo hashing": constant-time lookup (and insertion).

Best-case is O(1).
    _Average_ expected performance of a hash table with a good hash function is
    betw. O(1) ~ O(log N), strong bias toward O(1).

Double hashing: use a different hash function to compute step size
    // Get the initial index for the key.
    size_t idx = hash(key, size) % table->size;
    // Get the step size.
    size_t step = hash2(key) % (table->size - 1) + 1;
           ^-- Interval depends on the _data_ (unlike probing), so collisions
               have different bucket sequences.

Design of redis dict.c:  https://news.ycombinator.com/item?id=4599740
    1. At every operation you perform there is a rehashing step.
    2. You have a function that you can call to perform a rehashing step.
    3. There is an higher level function where you can say "rehash for
       N milliseconds" in case you have some idle time.
    Another uncommon thing of dict.c is that it supports the "pick a random
    element" operation with guaranteed O(1) behaviour.
    * We use both chaining and incremental rehashing. This means that there is
      no need to rehash ASAP when the hash table reached the max % of elements
      allowed. At the same time using the incremental rehashing there is no need
      to block.
    * We support safe and unsafe iterators: safe iterators block the rehashing
      when they are active, so you have the feeling you are accessing the hash
      table in a read-only way. Unsafe iterators are ok for looping even while
      an incremental rehashing is in progress, but is not ok to touch the hash
      table while iterating with an unsafe iterator (otherwise the iterator no
      longer guarantees to return all the elements, or to avoid duplicated
      elements).
    * The hash table is rehashed both ways: if it's too full, or if it's too
      empty. This way there is the guarantee that a table has always
      a percentage between a min and max. This guarantees that our random
      element function is in the average case O(1).
    * When we rehash, a second table is created, and stuff are moved
      incrementally from one to the other. When we have two tables, all the
      lookup / delete operations are run against both tables because the element
      can be in either table. Insertions only happen in the new table.

MORE
================================================================================
general purpose hash function
    MurmurHash3 C port:   https://github.com/PeterScott/murmur3
    MurmurHash3:          https://github.com/aappleby/smhasher/blob/master/src/MurmurHash3.cpp
    MurmurHash2 in redis: https://github.com/antirez/redis/blob/0c05436cef6f65cb2d62c8764522abefeb964314/src/dict.c#L92

string hash function:
    https://github.com/amadvance/tommyds
    https://github.com/torch/luajit-rocks/commit/cafad2e668764467d7846719ee964172c4e74b0b
